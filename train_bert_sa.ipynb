{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitdeeplearningcondadc48174f7e5745e3ac35d1f58f45bd65",
   "display_name": "Python 3.7.6 64-bit ('deeplearning': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Self-Attention Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import FloatTensor, LongTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "\n",
    "from layers import SelfAttention\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from transformers import BertModel, BertTokenizer, DistilBertTokenizer,\\\n",
    "    DistilBertModel\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def get_lens(batch: torch.tensor) -> torch.tensor:\n",
    "    batch = batch.detach().cpu()\n",
    "    lens = [len(np.where(row>0)[0]) for row in batch]\n",
    "    return torch.tensor(lens)\n",
    "\n",
    "class BertAttentionClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_classes: int):\n",
    "        super(BertAttentionClassifier, self).__init__()\n",
    "    \n",
    "        self.bert = BertModel.from_pretrained('bb_lm_ft/')\n",
    "        self.num_classes = num_classes\n",
    "        self.linear1 = nn.Linear(self.bert.config.hidden_size, 256)\n",
    "        self.self_attention = SelfAttention(256,batch_first=True, non_linearity=\"tanh\")\n",
    "        self.out = nn.Linear(256, num_classes)\n",
    "                    \n",
    "    def forward(self, \n",
    "               input_ids: torch.tensor,\n",
    "               sent_lengths: List[int]):\n",
    "        h, attn = self.bert(input_ids=input_ids)\n",
    "        linear1 = torch.nn.functional.relu(self.linear1(h))        \n",
    "        attention, _ = self.self_attention(linear1, sent_lengths)\n",
    "        logits = self.out(attention)\n",
    "        return logits, attn\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        input_ids, labels = batch\n",
    "        sent_lengths = get_lens(input_ids)\n",
    "        \n",
    "        # predict\n",
    "        y_hat, attn = self.forward(input_ids, sent_lengths)\n",
    "        \n",
    "        # loss \n",
    "        loss = F.cross_entropy(y_hat, labels)\n",
    "\n",
    "        # logs\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        sent_lengths = get_lens(input_ids)\n",
    "        \n",
    "        y_hat, attn = self.forward(input_ids, sent_lengths)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, labels)\n",
    "        \n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        labels = labels.cpu()\n",
    "\n",
    "        val_acc = accuracy_score(labels, y_hat)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        \n",
    "        val_f1 = f1_score(labels, y_hat, average='micro')\n",
    "        val_f1 = torch.tensor(val_f1)\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'val_f1': val_f1}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "        \n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc, 'avg_val_f1': avg_val_f1}\n",
    "        \n",
    "        return {'avg_val_loss': avg_loss, 'avg_val_f1':avg_val_f1 ,'progress_bar': tensorboard_logs}\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch\n",
    "        sent_lengths = get_lens(input_ids)\n",
    "        \n",
    "        y_hat, attn = self.forward(input_ids, sent_lengths)\n",
    "        \n",
    "        loss = F.cross_entropy(y_hat, labels)\n",
    "        \n",
    "        a, y_hat = torch.max(y_hat, dim=1)\n",
    "        y_hat = y_hat.cpu()\n",
    "        labels = labels.cpu()\n",
    "\n",
    "        val_acc = accuracy_score(labels, y_hat)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        \n",
    "        val_f1 = f1_score(labels, y_hat, average='micro')\n",
    "        val_f1 = torch.tensor(val_f1)\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': val_acc, 'val_f1': val_f1}\n",
    "    \n",
    "    def test_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        avg_val_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n",
    "        \n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc, 'avg_val_f1': avg_val_f1}\n",
    "        \n",
    "        return {'avg_val_loss': avg_loss, 'avg_val_f1':avg_val_f1 ,'progress_bar': tensorboard_logs}\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], \n",
    "                                lr=2e-05, eps=1e-08)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader_\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader_\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return test_dataloader_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "INFO:transformers.tokenization_utils:Model name 'bb_lm_ft/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bb_lm_ft/' is a path, a model identifier, or url to a directory containing tokenizer files.\nINFO:transformers.tokenization_utils:Didn't find file bb_lm_ft/added_tokens.json. We won't load it.\nINFO:transformers.tokenization_utils:loading file bb_lm_ft/vocab.txt\nINFO:transformers.tokenization_utils:loading file None\nINFO:transformers.tokenization_utils:loading file bb_lm_ft/special_tokens_map.json\nINFO:transformers.tokenization_utils:loading file bb_lm_ft/tokenizer_config.json\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from typing import List\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "dat = pd.read_csv(\"data/task_2_data.csv\")\n",
    "le = LabelEncoder()\n",
    "\n",
    "train = dat[dat[\"source\"]==\"train\"]\n",
    "dev = dat[dat[\"source\"]!=\"train\"]\n",
    "\n",
    "le = le.fit(train[\"label\"])\n",
    "train[\"encoded_label\"] = le.fit_transform(train[\"label\"]) \n",
    "train[\"num_words\"] = train[\"text\"].apply(lambda x: len(x.split()))\n",
    "\n",
    "random_seed = 1956\n",
    "tokenizer = BertTokenizer.from_pretrained('bb_lm_ft/')\n",
    "\n",
    "train, val = train_test_split(train, test_size=.15,\n",
    "                              stratify=train[\"encoded_label\"],\n",
    "                              random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 24\n",
    "\n",
    "X_train = [torch.tensor(tokenizer.encode(text)) for text in train[\"text\"]]\n",
    "X_train = pad_sequence(X_train, batch_first=True, padding_value=0)\n",
    "y_train = torch.tensor(train[\"encoded_label\"].tolist())\n",
    "\n",
    "X_val = [torch.tensor(tokenizer.encode(text)) for text in val[\"text\"]]\n",
    "X_val = pad_sequence(X_val, batch_first=True, padding_value=0)\n",
    "y_val = torch.tensor(val[\"encoded_label\"].tolist())\n",
    "\n",
    "ros = RandomOverSampler(random_state=random_seed)\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train_resampled = torch.tensor(X_train_resampled)\n",
    "y_train_resampled = torch.tensor(y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_resampled, y_train_resampled)\n",
    "train_dataloader_ = DataLoader(train_dataset,\n",
    "                               sampler=RandomSampler(train_dataset),\n",
    "                               batch_size=BATCH_SIZE)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_dataloader_ = DataLoader(val_dataset,\n",
    "                             sampler=RandomSampler(val_dataset),\n",
    "                             batch_size=BATCH_SIZE)\n",
    "\n",
    "dev_ids = [torch.tensor(tokenizer.encode(text)) for text in dev[\"text\"]]\n",
    "dev_ids = pad_sequence(dev_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "dev_dataset = TensorDataset(dev_ids)\n",
    "dev_dataloader_ = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([24, 192]) torch.Size([24])\n"
    }
   ],
   "source": [
    "for batch in train_dataloader_:\n",
    "  print(batch[0].shape, batch[1].shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "INFO:transformers.configuration_utils:loading configuration file bb_lm_ft/config.json\nINFO:transformers.configuration_utils:Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"do_sample\": false,\n  \"eos_token_ids\": 0,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 2,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 30522\n}\n\nINFO:transformers.modeling_utils:loading weights file bb_lm_ft/pytorch_model.bin\nINFO:root:gpu available: True, used: True\nINFO:root:VISIBLE GPUS: 0\nINFO:root:\n                                      Name            Type Params\n0                                     bert       BertModel  109 M\n1                          bert.embeddings  BertEmbeddings   23 M\n2          bert.embeddings.word_embeddings       Embedding   23 M\n3      bert.embeddings.position_embeddings       Embedding  393 K\n4    bert.embeddings.token_type_embeddings       Embedding    1 K\n..                                     ...             ...    ...\n216                                linear1          Linear  196 K\n217                         self_attention   SelfAttention  256  \n218                 self_attention.softmax         Softmax    0  \n219           self_attention.non_linearity            Tanh    0  \n220                                    out          Linear    3 K\n\n[221 rows x 3 columns]\nEpoch 1:  83%|████████▎ | 5/6 [00:03<00:00,  1.45batch/s, batch_idx=4, gpu=0, loss=2.482, v_num=4]\nEpoch 1: 100%|██████████| 6/6 [00:03<00:00,  1.45batch/s, avg_val_acc=0.5, avg_val_f1=0.5, batch_idx=4, gpu=0, loss=2.482, v_num=4, val_loss=2.12]\nEpoch 1: 100%|██████████| 6/6 [00:05<00:00,  1.15batch/s, avg_val_acc=0.5, avg_val_f1=0.5, batch_idx=4, gpu=0, loss=2.482, v_num=4, val_loss=2.12]\nfinished training\nsavin model\n"
    }
   ],
   "source": [
    "# model = BertAttentionClassifier(num_classes=14)\n",
    "\n",
    "# trainer = pl.Trainer(gpus=1, \n",
    "#                      max_epochs=1,\n",
    "#                      default_save_path=\".pl_bert_sa_logs/\")\n",
    "# trainer.fit(model)\n",
    "\n",
    "# print(\"finished training\")\n",
    "# print(\"savin model\")\n",
    "# torch.save(model.state_dict(), \"bb_sa_lm.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "INFO:transformers.configuration_utils:loading configuration file bb_lm_ft/config.json\nINFO:transformers.configuration_utils:Model config BertConfig {\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"do_sample\": false,\n  \"eos_token_ids\": 0,\n  \"finetuning_task\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"is_decoder\": false,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"length_penalty\": 1.0,\n  \"max_length\": 20,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_beams\": 1,\n  \"num_hidden_layers\": 12,\n  \"num_labels\": 2,\n  \"num_return_sequences\": 1,\n  \"output_attentions\": false,\n  \"output_hidden_states\": false,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"pruned_heads\": {},\n  \"repetition_penalty\": 1.0,\n  \"temperature\": 1.0,\n  \"top_k\": 50,\n  \"top_p\": 1.0,\n  \"torchscript\": false,\n  \"type_vocab_size\": 2,\n  \"use_bfloat16\": false,\n  \"vocab_size\": 30522\n}\n\nINFO:transformers.modeling_utils:loading weights file bb_lm_ft/pytorch_model.bin\n100%|██████████| 45/45 [01:21<00:00,  1.80s/it]finished\n\n"
    }
   ],
   "source": [
    "from utils import generate_t2_submission\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = BertAttentionClassifier(num_classes=14)\n",
    "model.load_state_dict(torch.load(\"bb_sa_lm.pt\"))\n",
    "model.eval()\n",
    "model.to(\"cpu\")\n",
    "\n",
    "all_preds = []\n",
    "for batch in tqdm(dev_dataloader_):\n",
    "    i = batch[0]\n",
    "    sl = get_lens(i)\n",
    "\n",
    "    preds, _ = model(i, sl)\n",
    "      \n",
    "    a, y_hat = torch.max(preds, dim=1)\n",
    "    y_hat = y_hat.cpu()\n",
    "    \n",
    "    all_preds.extend(y_hat)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def generate_t2_sub(preds: List[str]) -> List[str]:\n",
    "    \"\"\" Take a list of prediction and update the TC template\n",
    "        with those predictions \"\"\"\n",
    "    with open(\"data/dev-task-TC-template.out\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    final = []\n",
    "    for i, line in enumerate(lines):\n",
    "        pred = preds[i].strip()\n",
    "        line = line.replace(\"?\", pred)\n",
    "        final.append(line)\n",
    "    \n",
    "    return final\n",
    "\n",
    "preds = le.inverse_transform(all_preds)\n",
    "\n",
    "lines = generate_t2_sub(preds)\n",
    "\n",
    "with open(\"submissions/bb_sa_lm_preds_t2.txt\", \"w\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-3.6668, -0.9791, -2.4421, -3.1846, -2.0220, -0.9879,  0.6122, -3.1605,\n          7.8311,  0.3629,  1.0154, -1.6361, -1.6982, -3.3624],\n        [-3.7051, -2.2085, -2.3973, -3.4145, -3.2419, -1.3998,  4.3762, -2.2037,\n          5.4715,  2.0114,  0.6773, -2.4220, -2.7661, -2.4518],\n        [-2.3218, -2.0301, -3.3197, -2.5521, -2.4829, -1.5881, -0.5952, -1.7798,\n          1.9647,  7.0162,  2.1267, -1.9072, -3.2837, -1.6825],\n        [-3.9650, -1.1671, -2.3636, -3.0723, -2.7098, -1.6617,  2.2528, -3.0903,\n          7.4692,  0.9137,  0.7633, -1.6863, -2.5879, -3.0991],\n        [-3.3977, -1.7206, -3.5181, -3.6897, -3.2311, -1.0395, -0.0298, -2.4877,\n          6.3027,  2.2526,  3.9654, -1.3684, -2.7997, -3.3794],\n        [-2.7979, -2.2817, -3.1292, -3.1799, -2.7487, -0.2284,  0.5271, -2.6144,\n          4.4566,  5.5592,  1.7827, -2.8251, -2.8130, -2.8017],\n        [-1.6488, -1.0604, -2.9936, -1.9140, -1.5754, -0.4984, -1.1813,  7.6305,\n         -0.9371, -0.7844,  1.8205, -0.3798, -2.1361, -1.4863]],\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}